<!doctype html>
<html>
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<title>reveal.js</title>

		<link rel="stylesheet" href="dist/reset.css">
		<link rel="stylesheet" href="dist/reveal.css">
		<link rel="stylesheet" href="dist/theme/black.css">

		<!-- Theme used for syntax highlighted code -->
		<link rel="stylesheet" href="plugin/highlight/monokai.css">
	</head>
	<body>
		<div class="reveal">
			<div class="slides">
				<section>
    <h2>What are Generative Pre-Trained Transformers</h2>
    <p class="fragment">Generative pre-trained transformers are a type of deep learning model that have been pre-trained on a large corpus of text data.</p>
    <p class="fragment">They are called "generative" because they can generate new text based on the patterns they have learned from the pre-training data.</p>
    <p class="fragment">The transformer architecture is based on the idea of self-attention, which allows the model to weigh the importance of different parts of the input when making predictions.</p>
</section>

				<section>
    <h2>Applications of Generative Pre-Trained Transformers</h2>
    <p class="fragment">This allows the model to better understand the context of the input and generate more coherent and natural-sounding text.</p>
    <p class="fragment">Pre-training the model on a large corpus of text data allows it to learn patterns and relationships in the data, which can then be fine-tuned for specific tasks.</p>
    <p class="fragment">Examples of such tasks include language translation and text summarization.</p>
</section>
<section>
    <h2>Using GPT-2 with the Hugging Face's Transformers library</h2>
    <p class="fragment">The provided code uses the HuggingFace's `transformers` library to easily use pre-trained models like GPT-2.</p>
    <p class="fragment">The code installs the `transformers` library, imports necessary classes and functions, loads the pre-trained GPT-2 model and tokenizer, tokenize the prompt and generates the next word(s) based on the prompt. </p>
    <p class="fragment">The code also decodes the generated word(s) and prints the generated text. </p>
    <p class="fragment">The GPT-2 model is a smaller version of GPT-3 and is capable of running on a standard computer or server with a limited amount of memory and computational resources.</p>
</section>

			</div>
		</div>

		<script src="dist/reveal.js"></script>
		<script src="plugin/notes/notes.js"></script>
		<script src="plugin/markdown/markdown.js"></script>
		<script src="plugin/highlight/highlight.js"></script>
		<script>
			// More info about initialization & config:
			// - https://revealjs.com/initialization/
			// - https://revealjs.com/config/
			Reveal.initialize({
				hash: true,

				// Learn about plugins: https://revealjs.com/plugins/
				plugins: [ RevealMarkdown, RevealHighlight, RevealNotes ]
			});
		</script>
	</body>
</html>
